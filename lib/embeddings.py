"""\nOpenAI Embedding Generator\n"""\nimport os\nimport time\nfrom typing import List, Optional\nfrom openai import OpenAI\n\n\nclass EmbeddingGenerator:\n    MODEL = "text-embedding-3-small"\n    DIMENSIONS = 1536\n    MAX_TOKENS_PER_REQUEST = 8000\n    MAX_REQUESTS_PER_MINUTE = 3000\n    \n    def __init__(self, api_key: Optional[str] = None):\n        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")\n        if not self.api_key:\n            raise ValueError("OpenAI API key required")\n        self.client = OpenAI(api_key=self.api_key)\n        self._request_count = 0\n        self._minute_start = time.time()\n    \n    def _rate_limit(self):\n        self._request_count += 1\n        if time.time() - self._minute_start > 60:\n            self._request_count = 0\n            self._minute_start = time.time()\n        if self._request_count >= self.MAX_REQUESTS_PER_MINUTE - 100:\n            sleep_time = 60 - (time.time() - self._minute_start)\n            if sleep_time > 0: time.sleep(sleep_time)\n            self._request_count = 0\n            self._minute_start = time.time()\n    \n    def generate_embedding(self, text: str) -> List[float]:\n        if not text or not text.strip():\n            return [0.0] * self.DIMENSIONS\n        max_chars = self.MAX_TOKENS_PER_REQUEST * 4\n        if len(text) > max_chars: text = text[:max_chars]\n        self._rate_limit()\n        response = self.client.embeddings.create(model=self.MODEL, input=text, dimensions=self.DIMENSIONS)\n        return response.data[0].embedding\n    \n    def generate_embeddings_batch(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n        embeddings = []\n        for i in range(0, len(texts), batch_size):\n            batch = texts[i:i + batch_size]\n            processed_batch = []\n            empty_indices = []\n            for j, text in enumerate(batch):\n                if text and text.strip():\n                    max_chars = self.MAX_TOKENS_PER_REQUEST * 4\n                    processed_batch.append(text[:max_chars] if len(text) > max_chars else text)\n                else:\n                    empty_indices.append(j)\n            self._rate_limit()\n            if processed_batch:\n                response = self.client.embeddings.create(model=self.MODEL, input=processed_batch, dimensions=self.DIMENSIONS)\n                batch_embeddings = [e.embedding for e in response.data]\n                result = []\n                embedding_idx = 0\n                for j in range(len(batch)):\n                    if j in empty_indices:\n                        result.append([0.0] * self.DIMENSIONS)\n                    else:\n                        result.append(batch_embeddings[embedding_idx])\n                        embedding_idx += 1\n                embeddings.extend(result)\n            else:\n                embeddings.extend([[0.0] * self.DIMENSIONS] * len(batch))\n        return embeddings\n    \n    @staticmethod\n    def prepare_company_text(name: str, domain: str) -> str:\n        parts = []\n        if name: parts.append(f"Company: {name}")\n        if domain: parts.append(f"Domain: {domain}")\n        return " | ".join(parts) if parts else ""\n    \n    @staticmethod\n    def prepare_contact_text(firstname: str, lastname: str, company: str) -> str:\n        parts = []\n        full_name = " ".join(filter(None, [firstname, lastname]))\n        if full_name: parts.append(f"Person: {full_name}")\n        if company: parts.append(f"Company: {company}")\n        return " | ".join(parts) if parts else ""\n    \n    @staticmethod\n    def prepare_signal_text(description: str, citation: str) -> str:\n        parts = []\n        if description: parts.append(description)\n        if citation: parts.append(f"Source: {citation}")\n        return " | ".join(parts) if parts else ""\n